{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8123b500-0fa7-4803-adff-fc1ed3c28c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103999c-6bb2-4911-be48-44e08f47f0c3",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This section prepares the MNIST dataset for training and testing:\n",
    "\n",
    "1. **Dataset Loading**:\n",
    "   - The MNIST dataset is loaded and split into training and testing sets.\n",
    "\n",
    "\n",
    "2. **Data Transformation**:\n",
    "   - Transformations are applied to normalize the images and convert them into tensors.\n",
    "\n",
    "\n",
    "3. **DataLoaders**:\n",
    "   - DataLoaders are created to handle the data in batches:\n",
    "     - The training DataLoader shuffles the data for better generalization.\n",
    "     - The testing DataLoader does not shuffle the data.\n",
    "\n",
    "\n",
    "4. **Class Labels**:\n",
    "   - The class labels (digits 0-9) are defined for reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9204884c-f4e5-421e-8e53-1a03bd70d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "print('Training set has {} instances'.format(len(train_dataset)))\n",
    "print('Validation set has {} instances'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8a2cd",
   "metadata": {},
   "source": [
    "## Visualizing Sample Images\n",
    "\n",
    "This section contains a function to display a few images from the dataset along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba62852-8dd8-43d1-b09f-865535a1cf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_images_from_dataset(dataset, num_images=5):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        image, label = dataset[i]\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        image = image * 0.5 + 0.5\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "show_images_from_dataset(train_dataset, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2296dc-e67e-4590-b463-f378ed573669",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "This section defines three different neural network models for classifying MNIST digits.\n",
    "\n",
    "1. **OneLayerMLP**:\n",
    "   - This is the simplest model with only one fully connected (linear) layer.\n",
    "   - The input images (28x28 pixels) are flattened into a vector of size 784 and directly mapped to 10 output classes (digits 0-9).\n",
    "   - The model uses `log_softmax` at the output layer to compute class probabilities in log scale.\n",
    "\n",
    "\n",
    "2. **TwoLayerMLP**:\n",
    "   - This model has two hidden layers to learn more complex patterns in the data.\n",
    "   - The first layer maps the input (784 features) to 128 neurons, and the second layer maps 128 neurons to 64 neurons.\n",
    "   - The final layer maps the 64 neurons to 10 output classes.\n",
    "   - ReLU activation is applied after each hidden layer to introduce non-linearity, which helps the model learn better.\n",
    "\n",
    "\n",
    "3. **MLPWithBatchNorm**:\n",
    "   - This model is similar to the TwoLayerMLP but includes batch normalization after each hidden layer.\n",
    "   - Batch normalization helps stabilize and speed up training by normalizing the inputs to each layer.\n",
    "   - The architecture includes:\n",
    "     - A first layer with 128 neurons followed by batch normalization and ReLU activation.\n",
    "     - A second layer with 64 neurons followed by batch normalization and ReLU activation.\n",
    "     - A final layer that maps to 10 output classes.\n",
    "   - This model is designed to improve performance and reduce overfitting compared to the TwoLayerMLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3caae194-9f98-4d17-b699-6540bebfd1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPWithBatchNorm(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one layer MLP\n",
    "class OneLayerMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneLayerMLP, self).__init__()\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return torch.log_softmax(self.fc(x), dim=1)\n",
    "\n",
    "#two layer MLP\n",
    "class TwoLayerMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "#two layer MLP with batch normalization\n",
    "class MLPWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWithBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "model = MLPWithBatchNorm()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc17b8-d1ce-4f41-9012-5453785dc67c",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "This section defines the loss function and optimizer used for training the model:\n",
    "\n",
    "1. **Loss Function**:\n",
    "   - The loss function used is `nn.CrossEntropyLoss`, which is commonly used for multi-class classification problems.\n",
    "   - It calculates the difference between the predicted class probabilities and the actual class labels.\n",
    "\n",
    "\n",
    "2. **Optimizer**:\n",
    "   - The optimizer used is `optim.SGD` (Stochastic Gradient Descent) with a learning rate of 0.01 and momentum of 0.9.\n",
    "   - Momentum helps accelerate convergence by dampening oscillations during optimization.\n",
    "   - An alternative optimizer, `torch.optim.Adam` (Adam) combines the benefits of momentum and adaptive learning rates.\n",
    "   - Both optimizers provide a similar accuracy during training and test using MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf80e91-1c77-4c8e-95b4-cd722490191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad9201-8e76-4e6e-915b-63f46a8d5f58",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section defines the training process for the model.\n",
    "\n",
    "1. **Inputs**:\n",
    "   - The function takes the model, training data loader, loss function, optimizer, and the number of epochs as inputs.\n",
    "\n",
    "\n",
    "2. **Training Loop**:\n",
    "   - For each epoch, the model processes the training data in batches.\n",
    "   - The optimizer clears the gradients, the model makes predictions, and the loss is calculated.\n",
    "   - Backpropagation is performed to compute gradients, and the optimizer updates the model's weights.\n",
    "\n",
    "\n",
    "3. **Metrics**:\n",
    "   - The function tracks the loss and accuracy for each epoch.\n",
    "   - Loss is calculated as the average loss over all batches in the epoch.\n",
    "   - Accuracy is calculated as the percentage of correctly predicted labels.\n",
    "\n",
    "\n",
    "4. **Visualization**:\n",
    "   - After training, the function plots graphs for training loss and accuracy over the epochs to visualize the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f38a5-fd0a-4c40-ac03-14365425ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2448, Accuracy: 93.19%\n",
      "Epoch 2, Loss: 0.0997, Accuracy: 96.95%\n",
      "Epoch 3, Loss: 0.0706, Accuracy: 97.80%\n",
      "Epoch 4, Loss: 0.0565, Accuracy: 98.27%\n",
      "Epoch 5, Loss: 0.0451, Accuracy: 98.60%\n",
      "Epoch 6, Loss: 0.0379, Accuracy: 98.80%\n",
      "Epoch 7, Loss: 0.0315, Accuracy: 98.97%\n",
      "Epoch 8, Loss: 0.0273, Accuracy: 99.17%\n",
      "Epoch 9, Loss: 0.0256, Accuracy: 99.19%\n",
      "Epoch 10, Loss: 0.0211, Accuracy: 99.36%\n",
      "Epoch 11, Loss: 0.0195, Accuracy: 99.33%\n",
      "Epoch 12, Loss: 0.0167, Accuracy: 99.48%\n",
      "Epoch 13, Loss: 0.0152, Accuracy: 99.55%\n",
      "Epoch 14, Loss: 0.0138, Accuracy: 99.55%\n",
      "Epoch 15, Loss: 0.0134, Accuracy: 99.62%\n",
      "Epoch 16, Loss: 0.0114, Accuracy: 99.67%\n",
      "Epoch 17, Loss: 0.0119, Accuracy: 99.62%\n",
      "Epoch 18, Loss: 0.0117, Accuracy: 99.62%\n"
     ]
    }
   ],
   "source": [
    "def train_model_with_metrics(model, train_loader, criterion, optimizer, epochs=20):\n",
    "\n",
    "    train_losses = [] # list to store loss after each epoch\n",
    "    train_accuracies = [] # list to store accuracy after each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 # total loss for this epoch\n",
    "        correct = 0 # total correct predictions\n",
    "        total = 0 # total number of images\n",
    "\n",
    "        # loop through the training data \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # clear gradient\n",
    "            outputs = model(inputs) # make predictions \n",
    "            loss = criterion(outputs, labels) # calulate loss\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "\n",
    "            running_loss += loss.item() # accumulate loss\n",
    "            \n",
    "            _, predicted = outputs.max(1) # get the predicted class\n",
    "            total += labels.size(0) # update total numer of samples\n",
    "            correct += predicted.eq(labels).sum().item() # count correct predictions\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader) # average loss for this epoch\n",
    "        epoch_accuracy = 100 * correct / total # calculate accuracy\n",
    "        train_losses.append(epoch_loss) \n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss graph\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy graph\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Accuracy', color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "train_model_with_metrics(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab060eca-a0df-4dee-934e-4c76d4dd04d5",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The `evaluate_model` function tests the model on the test dataset and calculates its performance:\n",
    "\n",
    "1. **Evaluation Mode**:\n",
    "   - The model is set to evaluation mode using `model.eval()` to disable training-specific features like dropout.\n",
    "\n",
    "\n",
    "2. **No Gradients**:\n",
    "   - The evaluation runs inside `torch.no_grad()` to save memory and computation.\n",
    "\n",
    "\n",
    "3. **Loop Through Test Data**:\n",
    "   - For each batch:\n",
    "     - Inputs and labels are moved to the device.\n",
    "     - The model predicts outputs, and the loss is calculated.\n",
    "     - Correct predictions are counted by comparing predicted and actual labels.\n",
    "\n",
    "\n",
    "4. **Metrics**:\n",
    "   - The average loss and accuracy are calculated and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f056e6-0fc6-485e-aac4-e672d0e2dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    correct = 0  # count of correct predictions\n",
    "    total = 0  # total number of samples\n",
    "    running_loss = 0.0  # total loss\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calculation for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # get model predictions\n",
    "            loss = criterion(outputs, labels)  # calculate loss\n",
    "            running_loss += loss.item()  # accumulate loss\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  # get the predicted class\n",
    "            total += labels.size(0)  # update the total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # count correct predictions\n",
    "\n",
    "    print(f\"Test Loss: {running_loss/len(test_loader)}\")\n",
    "    print(f\"Test Accuracy: {100 * correct / total}%\")\n",
    "    \n",
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973eb1e3-25ac-48d5-8e3b-d047eb02c1fd",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "The `plot_confusion_matrix` function visualizes the model's performance by displaying a confusion matrix:\n",
    "\n",
    "1. **Collect Predictions**:\n",
    "   - The function iterates through the test data without calculating gradients (`torch.no_grad()`).\n",
    "   - For each batch, it collects the model's predictions and the actual labels.\n",
    "\n",
    "\n",
    "2. **Create Confusion Matrix**:\n",
    "   - A confusion matrix is generated using `confusion_matrix` from `sklearn.metrics`.\n",
    "   - It shows how often each class was correctly or incorrectly predicted.\n",
    "\n",
    "This visualization helps identify which classes the model struggles with and where improvements can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272dfbaf-a483-47bf-8e98-46f52eb81103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, test_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[str(i) for i in range(10)])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "plot_confusion_matrix(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85807a4d-3252-49b6-88a5-c44a8f9c3ce1",
   "metadata": {},
   "source": [
    "## Examine Misclassified Data\n",
    "\n",
    "This section identifies and visualizes the images that the model predicted incorrectly:\n",
    "\n",
    "1. **Collect Misclassified Images**:\n",
    "   - For each batch, the function compares the model's predictions with the actual labels.\n",
    "   - If a prediction is incorrect, the corresponding image, predicted label, and actual label are stored.\n",
    "\n",
    "\n",
    "2. **Visualize Misclassified Images**:\n",
    "   - A few misclassified images are displayed with its predicted label and the correct label for easy comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a85a47-717c-4eb2-9429-948e5f6e82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        for img, pred, label in zip(inputs, preds, labels):\n",
    "            if pred != label:\n",
    "                misclassified.append((img.cpu(), pred.cpu(), label.cpu()))\n",
    "\n",
    "for img, pred, label in misclassified[:10]:\n",
    "    plt.imshow(img.permute(1, 2, 0).numpy() / 2 + 0.5)\n",
    "    plt.title(f'Predicted: {classes[pred]} | Actual: {classes[label]}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfb077-3c3d-4ba8-9ba9-68e5ee271d77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
